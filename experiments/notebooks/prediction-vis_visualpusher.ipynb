{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, time\n",
    "sys.path.append('../..')\n",
    "from utils import set_seed_torch, rgb2gray\n",
    "set_seed_torch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d\n",
    "        \n",
    "args = ObjectView({'res': 64,\n",
    " 'dataset_path': '/home/olimoyo/visual-haptic-dynamics/experiments/data/datasets/{}'\n",
    "                   .format(\"visual_haptic_2D_len16_569B46785E3F45BCA172AE53EA070D5E.pkl\"),\n",
    " 'models_dir': '/home/olimoyo/visual-haptic-dynamics/saved_models/{}'\n",
    "                   .format(\"test\"),\n",
    " 'device': 'cuda:0',\n",
    " 'n_examples': 5,\n",
    " 'n_predictions': 6,\n",
    "})\n",
    "\n",
    "def load_models_dir(models_dir):\n",
    "    \"\"\"Load hyperparameters from trained model.\"\"\"\n",
    "    dict_of_models = {}\n",
    "    for filedir in os.listdir(models_dir):\n",
    "        fullpath = os.path.join(models_dir, filedir)\n",
    "        if os.path.isdir(fullpath):\n",
    "            with open(os.path.join(fullpath, 'hyperparameters.txt'), 'r') as fp:\n",
    "                dict_of_models[fullpath] = Namespace(**json.load(fp))\n",
    "    return dict_of_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.dataset_path, 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "imgs_rgb = torch.from_numpy(data[\"img\"]).to(device=args.device).float()\n",
    "imgs_gray = torch.from_numpy(rgb2gray(data[\"img\"]).transpose(0, 1, 4, 2, 3)).to(device=args.device).float()\n",
    "ft = torch.from_numpy(data[\"ft\"]).to(device=args.device).float()\n",
    "arm = torch.from_numpy(data[\"arm\"]).to(device=args.device).float()\n",
    "actions = torch.from_numpy(data[\"action\"]).to(device=args.device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_models, load_vh_models, frame_stack\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict_of_models = load_models_dir(args.models_dir)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path, hyperparams in dict_of_models.items():\n",
    "        model_args = hyperparams\n",
    "        if model_args.dim_x[0] == 1:\n",
    "            imgs = imgs_gray\n",
    "        elif model_args.dim_x[0] == 3:\n",
    "            imgs = imgs_rgb\n",
    "        ii = np.random.randint(imgs.shape[0] // args.n_examples)\n",
    "        total_len = imgs.shape[1]\n",
    "        T = model_args.frame_stacks\n",
    "        start_idx = np.random.randint(total_len - args.n_predictions - T)\n",
    "\n",
    "        u_f = actions[:, start_idx:][\n",
    "            args.n_examples*ii:args.n_examples*(ii+1), \n",
    "            (T + 1):(T + 1 + args.n_predictions)\n",
    "        ] \n",
    "        \n",
    "        x = imgs[:, start_idx:][\n",
    "            args.n_examples*ii:args.n_examples*(ii+1), \n",
    "            :(T + 1 + args.n_predictions)\n",
    "        ]\n",
    "        x_i = imgs[:, start_idx:][\n",
    "            args.n_examples*ii:args.n_examples*(ii+1), \n",
    "            :(T + 1)\n",
    "        ] \n",
    "        x_i = frame_stack(x_i, frames=T)  \n",
    "\n",
    "        x_ft = ft[:, start_idx:][\n",
    "            args.n_examples*ii:args.n_examples*(ii+1), \n",
    "            :(T + 1 + args.n_predictions)\n",
    "        ]\n",
    "        x_ft_i = ft[:, start_idx:][\n",
    "            args.n_examples*ii:args.n_examples*(ii+1), \n",
    "            T:(T + 1)\n",
    "        ]\n",
    "\n",
    "        x_arm = arm[:, start_idx:][\n",
    "            args.n_examples*ii:args.n_examples*(ii+1), \n",
    "            :(T + 1 + args.n_predictions)\n",
    "        ]\n",
    "        x_arm_i = arm[:, start_idx:][\n",
    "            args.n_examples*ii:args.n_examples*(ii+1), \n",
    "            T:(T + 1)\n",
    "        ]\n",
    "\n",
    "        models = load_vh_models(path, model_args, mode='eval', device=args.device)\n",
    "        \n",
    "        # Roll-out or predictions\n",
    "        z_all = []\n",
    "        if model_args.use_img_enc:\n",
    "            z_all.append(models[\"img_enc\"](x_i.reshape(-1, *x_i.shape[2:])))\n",
    "\n",
    "        if model_args.use_joint_enc:\n",
    "            joint_inp = torch.cat((\n",
    "                x_ft_i.reshape(-1, *x_ft_i.shape[2:]), \n",
    "                x_arm_i.reshape(-1, *x_arm_i.shape[2:])), \n",
    "                dim=-1\n",
    "            )\n",
    "            z_all.append(models[\"joint_enc\"](joint_inp)[:, -1])\n",
    "        else:\n",
    "            if model_args.use_haptic_enc:\n",
    "                z_all.append(models[\"haptic_enc\"](x_ft_i.reshape(-1, *x_ft_i.shape[2:]))[:, -1])\n",
    "            if model_args.use_arm_enc:\n",
    "                z_all.append(models[\"arm_enc\"](x_arm_i.reshape(-1, *x_arm_i.shape[2:]))[:, -1])\n",
    "            \n",
    "        z_cat_i = torch.cat(z_all, dim=1)\n",
    "        z_i, mu_z_i, logvar_z_i = models[\"mix\"](z_cat_i)\n",
    "        var_z_i = torch.diag_embed(torch.exp(logvar_z_i))\n",
    "        h_i = None\n",
    "\n",
    "        z_hat = torch.zeros((args.n_examples, (1 + args.n_predictions), model_args.dim_z)).to(device=args.device)\n",
    "        z_hat[:, 0] = z_i\n",
    "\n",
    "        for jj in range(args.n_predictions):\n",
    "            z_ip1, mu_z_ip1, var_z_ip1, h_ip1 = models[\"dyn\"](\n",
    "                z_t=z_i, \n",
    "                mu_t=mu_z_i, \n",
    "                var_t=var_z_i, \n",
    "                u=u_f[:, jj], \n",
    "                h=h_i, \n",
    "                single=True\n",
    "            )\n",
    "\n",
    "            z_hat[:, jj + 1] = mu_z_ip1\n",
    "            z_i, mu_z_i, var_z_i, h_i = z_ip1, mu_z_ip1, var_z_ip1, h_ip1    \n",
    "        \n",
    "        # Decode predictions\n",
    "        x_hat = models[\"img_dec\"](z_hat.reshape(-1, *z_hat.shape[2:]))\n",
    "        x_hat = x_hat.reshape(args.n_examples, (args.n_predictions + 1), *x_hat.shape[1:])\n",
    "            \n",
    "        # Move to cpu, np\n",
    "        x_hat = x_hat.cpu().numpy()\n",
    "        x = x.cpu().numpy()\n",
    "        \n",
    "        mse = np.sum((x_hat[:, 1:, 0].reshape(args.n_examples, args.n_predictions, -1) - \n",
    "                      x[:, 2:, 0].reshape(args.n_examples, args.n_predictions, -1))**2, axis=2)\n",
    "\n",
    "        # Plotting\n",
    "        for bb in range(args.n_examples):\n",
    "            columns = T + 1 + args.n_predictions\n",
    "            rows = 2\n",
    "            fig=plt.figure(figsize=(16, 2))\n",
    "            fig.suptitle('frame_stacks = {}, predictions = {}'.format(T, args.n_predictions))\n",
    "            for ii in range(columns*rows):\n",
    "                if ii<((columns*rows)/2):\n",
    "                    img = x[bb,ii,0,:,:]\n",
    "                else:\n",
    "                    idx = int(ii-((columns*rows)/2))\n",
    "                    if idx < T:\n",
    "                        img = np.zeros((model_args.dim_x[1], model_args.dim_x[2]))\n",
    "                    else:\n",
    "                        img = x_hat[bb,idx-1,0,:,:]\n",
    "                fig.add_subplot(rows, columns, ii+1)\n",
    "                plt.imshow(img, cmap=\"gray\")\n",
    "                \n",
    "                plt.axis('off')\n",
    "            print(\"MSE of predictions: \", mse[bb])\n",
    "            plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
