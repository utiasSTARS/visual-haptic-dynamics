{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, time\n",
    "sys.path.append('../..')\n",
    "from utils import set_seed_torch, rgb2gray\n",
    "set_seed_torch(3)\n",
    "from argparse import Namespace\n",
    "import json\n",
    "from utils import load_models, load_vh_models, frame_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d\n",
    "        \n",
    "args = ObjectView({\n",
    " 'res': 64,\n",
    " 'dataset_path': '/home/olimoyo/visual-haptic-dynamics/experiments/data/datasets/{}'\n",
    "                   .format(\"visual_haptic_2D_len16_osc_withGT_8C12919B740845539C0E75B5CBAF7965.pkl\"),\n",
    " 'models_dir': '/home/olimoyo/visual-haptic-dynamics/saved_models/{}'\n",
    "                   .format(\"vaughan/test/\"),\n",
    " 'device': 'cuda:1',\n",
    " 'n_examples': 3,\n",
    " 'n_pred': 8,\n",
    " 'n_initial': 5\n",
    "})\n",
    "\n",
    "def load_models_dir(models_dir):\n",
    "    \"\"\"Load hyperparameters from trained model.\"\"\"\n",
    "    dict_of_models = {}\n",
    "    for filedir in os.listdir(models_dir):\n",
    "        fullpath = os.path.join(models_dir, filedir)\n",
    "        if os.path.isdir(fullpath):\n",
    "            with open(os.path.join(fullpath, 'hyperparameters.txt'), 'r') as fp:\n",
    "                dict_of_models[fullpath] = Namespace(**json.load(fp))\n",
    "    return dict_of_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.dataset_path, 'rb') as f:\n",
    "    raw_data = pkl.load(f)\n",
    "\n",
    "data = {}\n",
    "data['img_rgb'] = torch.from_numpy(raw_data[\"img\"].transpose(0, 1, 4, 2, 3)).int().to(device=args.device)\n",
    "data['img_gray'] = torch.from_numpy(rgb2gray(raw_data[\"img\"]).transpose(0, 1, 4, 2, 3)).float().to(device=args.device)\n",
    "data['haptic'] = torch.from_numpy(raw_data['ft']).float().to(device=args.device) / 100.0\n",
    "data['arm'] = torch.from_numpy(raw_data['arm']).float().to(device=args.device)\n",
    "\n",
    "data['actions'] = torch.from_numpy(raw_data[\"action\"]).to(device=args.device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models in path:  /home/olimoyo/visual-haptic-dynamics/saved_models/vaughan/test/allpaststates_z16_net512_l0_lm_osc_v_r0.95_kl0.80_lr3e4\n",
      "wqe torch.Size([3, 4, 16])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "z hat torch.Size([3, 12, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b52d9ef4204a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mz_all_dec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_img_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mz_cat_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_all_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img_dec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_cat_dec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mz_cat_dec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "dict_of_models = load_models_dir(args.models_dir)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path, model_args in dict_of_models.items():\n",
    "            \n",
    "        nets = load_vh_models(path=path, args=model_args, mode='eval', device=args.device)\n",
    "        \n",
    "        def encode(nets, x_img, x_ft, x_arm, ctx_img):\n",
    "            if model_args.context_modality != \"none\":\n",
    "                if model_args.context_modality == \"joint\": \n",
    "                    ctx = torch.cat((x_ft, x_arm), dim=-1) # (n, l, f, 12)\n",
    "                elif model_args.context_modality == \"ft\": \n",
    "                    ctx = x_ft\n",
    "                elif model_args.context_modality == \"arm\":\n",
    "                    ctx = x_arm\n",
    "                ctx = ctx.float().to(device=args.device) # (n, l, f, 6)\n",
    "                ctx = ctx.transpose(-1, -2)\n",
    "                ctx = ctx.reshape(-1, *ctx.shape[2:])\n",
    "            n, l = x_img.shape[0], x_img.shape[1]\n",
    "\n",
    "            if model_args.context in [\"initial_image\", \"goal_image\"]:\n",
    "                x_img = torch.cat((x_img, ctx_img.repeat(1, l, 1, 1, 1)), dim=2)\n",
    "            \n",
    "            z_all_enc = []\n",
    "            z_img = nets[\"img_enc\"](x_img.reshape(-1, *x_img.shape[2:]))\n",
    "            z_all_enc.append(z_img)  \n",
    "\n",
    "            if model_args.context_modality != \"none\":\n",
    "                z_context = nets[\"context_enc\"](ctx)\n",
    "                z_all_enc.append(z_context)   \n",
    "            \n",
    "            if model_args.context in [\"initial_latent_state\", \"goal_latent_state\"]:\n",
    "                z_img_context = nets[\"context_img_enc\"](ctx_img)\n",
    "                ret_context = z_img_context\n",
    "                z_img_context_rep = z_img_context.unsqueeze(1).repeat(1, l, 1)\n",
    "                z_all_enc.append(z_img_context_rep.reshape(-1, *z_img_context_rep.shape[2:]))        \n",
    "            elif model_args.context in [\"all_past_states\"]:\n",
    "                if l > 1:\n",
    "                    z_img_context, h_img_context = nets[\"context_img_rnn_enc\"](\n",
    "                        z_img.reshape(n, l, *z_img.shape[1:])[:, :-1].transpose(1,0)\n",
    "                    )\n",
    "                    pad = torch.zeros((1, *z_img_context.shape[1:])).float().to(device=args.device)\n",
    "                    z_img_context = torch.cat((pad, z_img_context), dim=0)\n",
    "                    z_img_context = z_img_context.transpose(1, 0)\n",
    "                    ret_context = (z_img_context, h_img_context)\n",
    "                else:\n",
    "                    z_img_context = torch.zeros((n, l, 16)).float().to(device=args.device)\n",
    "                    ret_context = (z_img_context, None)\n",
    "                z_all_enc.append(z_img_context.reshape(-1, *z_img_context.shape[2:]))\n",
    "            else:\n",
    "                ret_context = None\n",
    "                        \n",
    "            z_cat_enc = torch.cat(z_all_enc, dim=-1)\n",
    "            z, mu_z, logvar_z = nets[\"mix\"](z_cat_enc)\n",
    "            var_z = torch.diag_embed(torch.exp(logvar_z))\n",
    "            \n",
    "            return z, mu_z, var_z, ret_context\n",
    "        \n",
    "        if model_args.dim_x[0] == 1:\n",
    "            img_key = 'img_gray'\n",
    "        elif model_args.dim_x[0] == 3:\n",
    "            img_key = 'img_rgb'\n",
    "                    \n",
    "        T = model_args.frame_stacks\n",
    "        \n",
    "        assert args.n_initial + args.n_pred <= data[img_key].shape[1]\n",
    "        assert args.n_initial >= T\n",
    "        \n",
    "        # Use a random batch to test\n",
    "        ii = np.random.randint(data[img_key].shape[0] // args.n_examples)\n",
    "        batch_range = range(args.n_examples*ii, args.n_examples*(ii+1))\n",
    "        test_batch = {k:v[batch_range] for k,v in data.items()}\n",
    "        \n",
    "        # Ground truth images and controls\n",
    "        x_img = test_batch[img_key][:, :(args.n_initial + args.n_pred)]\n",
    "        u = test_batch['actions']\n",
    "        \n",
    "        # Sequence of initial images\n",
    "        x_img_i = x_img[:, :args.n_initial]\n",
    "        x_img_i = frame_stack(x_img_i, frames=T)\n",
    "        \n",
    "        # Sequence of ground truth images\n",
    "        x_img_gt = x_img[:, -(T + 1):]\n",
    "        x_img_gt = frame_stack(x_img_gt, frames=T)\n",
    "        \n",
    "        # Sequence of extra modalities\n",
    "        x_ft_i = test_batch['haptic'][:, T:args.n_initial]\n",
    "        x_arm_i = test_batch['arm'][:, T:args.n_initial]\n",
    "\n",
    "        n, l = x_img_i.shape[0], x_img_i.shape[1] \n",
    "\n",
    "        if model_args.context in [\"initial_latent_state\", \"initial_image\"]:\n",
    "            ctx_img = x_img_i[:, 0]\n",
    "        elif model_args.context in [\"goal_latent_state\", \"goal_image\"]:\n",
    "            ctx_img = x_img_gt[:, 0]\n",
    "        else:\n",
    "            ctx_img = None\n",
    "        \n",
    "        # Encode\n",
    "        z_i, mu_z_i, var_z_i, ret_context = encode(nets, x_img_i, x_ft_i, x_arm_i, ctx_img)\n",
    "        if model_args.context in [\"all_past_states\"]:\n",
    "            z_img_context, h_img_context = ret_context\n",
    "        else:\n",
    "            z_img_context = ret_context\n",
    "        h_i = None\n",
    "                \n",
    "        # Group and prepare for prediction\n",
    "        q_z_i = {\"z\": z_i, \"mu\": mu_z_i, \"cov\": var_z_i}\n",
    "        q_z_i = {k:v.reshape(n, l, *v.shape[1:]).transpose(1,0) for k, v in q_z_i.items()}\n",
    "        u = u.transpose(1,0)\n",
    "\n",
    "        z_hat = torch.zeros(((l + args.n_pred), n, model_args.dim_z)).to(device=args.device)\n",
    "        z_hat[0:l] = q_z_i[\"mu\"]\n",
    "        \n",
    "        # First run\n",
    "        z_i, mu_z_i, var_z_i = q_z_i[\"z\"], q_z_i[\"mu\"], q_z_i[\"cov\"]\n",
    "        u_i = u[(T + 1):(1 + args.n_initial)]\n",
    "        \n",
    "        print(\"wqe\", z_img_context.shape)\n",
    "\n",
    "        # Predict\n",
    "        for jj in range(0, args.n_pred):\n",
    "            z_ip1, mu_z_ip1, var_z_ip1, h_ip1 = nets[\"dyn\"](\n",
    "                z_t=z_i, \n",
    "                mu_t=mu_z_i, \n",
    "                var_t=var_z_i, \n",
    "                u=u_i, \n",
    "                h_0=h_i, \n",
    "                single=False\n",
    "            )\n",
    "            z_hat[jj + l] = mu_z_ip1[-1]\n",
    "            z_i, mu_z_i, var_z_i, h_i = z_ip1[-1:], mu_z_ip1[-1:], var_z_ip1[-1:], h_ip1\n",
    "            u_i = u[1 + args.n_initial + jj][None]\n",
    "            print(jj)\n",
    "            if model_args.context in [\"all_past_states\"]:\n",
    "                z_img_context_ip1 = z_img_context[:, -1]\n",
    "                z_cat_single_dec = torch.cat((mu_z_ip1[-1], z_img_context_ip1), dim=-1)\n",
    "                x_hat_ip1 = nets[\"img_dec\"](z_cat_single_dec)\n",
    "                z_img_ip1 = nets[\"img_enc\"](x_hat_ip1)\n",
    "                z_img_context_ip1, h_img_context = nets[\"context_img_rnn_enc\"](z_img_ip1.unsqueeze(0), h=h_img_context)\n",
    "                z_img_context = torch.cat((z_img_context, z_img_context_ip1.transpose(1,0)), dim=1)\n",
    "                #TODO: Accumulate context for 12 steps\n",
    "                \n",
    "        z_hat = z_hat.transpose(1, 0)\n",
    "        print(\"z hat\", z_hat.shape)\n",
    "        \n",
    "        # Decode\n",
    "        z_all_dec = []\n",
    "        z_all_dec.append(z_hat)\n",
    "\n",
    "        if model_args.context in [\"initial_latent_state\", \"goal_latent_state\"]:\n",
    "            z_img_context_rep = z_img_context.unsqueeze(1).repeat(1, (l + args.n_pred), 1)\n",
    "            z_all_dec.append(z_img_context_rep)\n",
    "        elif model_args.context in [\"all_past_states\"]:\n",
    "            z_all_dec.append(z_img_context[:, -1])\n",
    "\n",
    "        z_cat_dec = torch.cat(z_all_dec, dim=-1)\n",
    "        x_hat = nets[\"img_dec\"](z_cat_dec.reshape(-1, *z_cat_dec.shape[2:]))\n",
    "        x_hat = x_hat.reshape(n, (l + args.n_pred), *x_hat.shape[1:])\n",
    "                \n",
    "        # Move to cpu, np\n",
    "        x_hat = x_hat.cpu().numpy()\n",
    "        x_img = x_img.cpu().numpy()\n",
    "\n",
    "        mse = np.sum((x_hat[:, l:, 0].reshape(n, args.n_pred, -1) - \n",
    "                      x_img[:, args.n_initial:, 0].reshape(n, args.n_pred, -1))**2, axis=2)\n",
    "\n",
    "        # Plotting\n",
    "        for bb in range(n):\n",
    "            columns = args.n_initial + args.n_pred\n",
    "            rows = 2\n",
    "            fig=plt.figure(figsize=(16, 2))\n",
    "            fig.suptitle('n_initial = {}, frame_stacks = {}, predictions = {}'.format(args.n_initial, T, args.n_pred))\n",
    "            for ii in range(columns*rows):\n",
    "                if ii<((columns*rows)/2):\n",
    "                    img = x_img[bb,ii,0,:,:]\n",
    "                else:\n",
    "                    idx = int(ii-((columns*rows)/2))\n",
    "                    if idx < T:\n",
    "                        img = np.zeros((model_args.dim_x[1], model_args.dim_x[2]))\n",
    "                    else:\n",
    "                        img = x_hat[bb,idx-1,0,:,:]\n",
    "                fig.add_subplot(rows, columns, ii+1)\n",
    "                plt.imshow(img, cmap=\"gray\")\n",
    "                \n",
    "                plt.axis('off')\n",
    "            print(\"MSE of predictions: \", mse[bb])\n",
    "            plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
