{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "import torch\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, time\n",
    "sys.path.append('../..')\n",
    "from utils import set_seed_torch, rgb2gray\n",
    "set_seed_torch(3)\n",
    "from argparse import Namespace\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d\n",
    "        \n",
    "args = ObjectView({'res': 64,\n",
    " 'dataset_path': '/home/olimoyo/visual-haptic-dynamics/experiments/data/datasets/{}'\n",
    "                   .format(\"visual_haptic_2D_len16_osc_withGT_8C12919B740845539C0E75B5CBAF7965.pkl\"),\n",
    " 'models_dir': '/home/olimoyo/visual-haptic-dynamics/saved_models/{}'\n",
    "                   .format(\"vaughan/osc\"),\n",
    " 'device': 'cuda:0',\n",
    " 'up_to_n_pred': 8,\n",
    "})\n",
    "\n",
    "def load_models_dir(models_dir):\n",
    "    \"\"\"Load hyperparameters from trained model.\"\"\"\n",
    "    dict_of_models = {}\n",
    "    for filedir in os.listdir(models_dir):\n",
    "        fullpath = os.path.join(models_dir, filedir)\n",
    "        if os.path.isdir(fullpath):\n",
    "            with open(os.path.join(fullpath, 'hyperparameters.txt'), 'r') as fp:\n",
    "                dict_of_models[fullpath] = Namespace(**json.load(fp))\n",
    "    return dict_of_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.dataset_path, 'rb') as f:\n",
    "    raw_data = pkl.load(f)\n",
    "\n",
    "data = {}\n",
    "data['img_rgb'] = torch.from_numpy(raw_data[\"img\"].transpose(0, 1, 4, 2, 3)).int().to(device=args.device)\n",
    "data['img_gray'] = torch.from_numpy(rgb2gray(raw_data[\"img\"]).transpose(0, 1, 4, 2, 3)).float().to(device=args.device)\n",
    "data['haptic'] = torch.from_numpy(raw_data['ft']).float().to(device=args.device) / 100.0\n",
    "data['arm'] = torch.from_numpy(raw_data['arm']).float().to(device=args.device)\n",
    "    \n",
    "actions = torch.from_numpy(raw_data[\"action\"]).to(device=args.device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_vh_models, frame_stack\n",
    "import torch.nn as nn\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models in path:  /home/olimoyo/visual-haptic-dynamics/saved_models/vaughan/osc/lm_ul_osc_vha_r0.95_kl0.95_lr3e4\n",
      "prediction length 1 starting position 1 mse img:  17.852303466796876 mse z:  57173.445\n",
      "\n",
      "prediction length 1 starting position 2 mse img:  8.76671630859375 mse z:  12711.30875\n",
      "\n",
      "prediction length 1 starting position 3 mse img:  5.1877899169921875 mse z:  3454.695\n",
      "\n",
      "prediction length 1 starting position 4 mse img:  4.636831359863281 mse z:  1133.79859375\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-885f52ae1463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mbatch_psnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                     batch_ssim += ssim(\n\u001b[0m\u001b[1;32m    137\u001b[0m                         \u001b[0mx_img_gt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                         \u001b[0mx_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt15env/lib/python3.8/site-packages/skimage/metrics/_structural_similarity.py\u001b[0m in \u001b[0;36mstructural_similarity\u001b[0;34m(im1, im2, win_size, gradient, data_range, multichannel, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# compute (weighted) variances and covariances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0muxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mim1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfilter_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0muyy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfilter_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0muxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfilter_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mvx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov_norm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muxx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mux\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt15env/lib/python3.8/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36muniform_filter\u001b[0;34m(input, size, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m             uniform_filter1d(input, int(size), axis, output, mode,\n\u001b[0m\u001b[1;32m    900\u001b[0m                              cval, origin)\n\u001b[1;32m    901\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt15env/lib/python3.8/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36muniform_filter1d\u001b[0;34m(input, size, axis, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid origin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_mode_to_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m     _nd_image.uniform_filter1d(input, size, axis, output, mode, cval,\n\u001b[0m\u001b[1;32m    841\u001b[0m                                origin)\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dict_of_models = load_models_dir(args.models_dir)\n",
    "analysis_data = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path, model_args in dict_of_models.items():\n",
    "        \n",
    "        model_name = path.split(\"/\")[-1]\n",
    "        nets = load_vh_models(path=path, args=model_args, mode='eval', device=args.device)\n",
    "\n",
    "        def encode(nets, x_img, x_ft, x_arm, ctx_img):\n",
    "            # Prepare context data depending on model\n",
    "            if model_args.context_modality != \"none\":\n",
    "                if model_args.context_modality == \"joint\": \n",
    "                    ctx = torch.cat((x_ft, x_arm), dim=-1) # (n, l, f, 12)\n",
    "                elif model_args.context_modality == \"ft\": \n",
    "                    ctx = x_ft\n",
    "                elif model_args.context_modality == \"arm\":\n",
    "                    ctx = x_arm\n",
    "                ctx = ctx.float().to(device=args.device) # (n, l, f, 6)\n",
    "                ctx = ctx.transpose(-1, -2)\n",
    "                ctx = ctx.reshape(-1, *ctx.shape[2:])  \n",
    "                    \n",
    "            # Encode\n",
    "            z_all_enc = []\n",
    "\n",
    "            z_img = nets[\"img_enc\"](x_img)\n",
    "            z_all_enc.append(z_img)              \n",
    "            \n",
    "            context = {}\n",
    "            if model_args.context_modality != \"none\":\n",
    "                z_context = nets[\"context_enc\"](ctx)\n",
    "                z_all_enc.append(z_context)\n",
    "                context[\"z_context\"] = z_context\n",
    "            if model_args.context==\"initial_latent_state\":\n",
    "                z_img_context = nets[\"context_img_enc\"](ctx_img)\n",
    "                z_all_enc.append(z_img_context)\n",
    "                context[\"z_img_context\"] = z_img_context\n",
    "\n",
    "            # Concatenate modalities and mix\n",
    "            z_cat_enc = torch.cat(z_all_enc, dim=1)\n",
    "            z, mu_z, logvar_z = nets[\"mix\"](z_cat_enc)\n",
    "            var_z = torch.diag_embed(torch.exp(logvar_z))\n",
    "            return z, mu_z, var_z, context\n",
    "        \n",
    "        if model_args.dim_x[0] == 1:\n",
    "            imgs = data['img_gray']\n",
    "        elif model_args.dim_x[0] == 3:\n",
    "            imgs = data['img_rgb']\n",
    "        \n",
    "        total_len = imgs.shape[1]\n",
    "        T = model_args.frame_stacks\n",
    "\n",
    "        analysis_data[model_name] = {}\n",
    "        for n_pred in range(1, args.up_to_n_pred + 1):\n",
    "            analysis_data[model_name][f\"{n_pred}_pred\"] = {}\n",
    "            analysis_data[model_name][f\"{n_pred}_pred\"][\"MSE_x\"] = 0\n",
    "            analysis_data[model_name][f\"{n_pred}_pred\"][\"MSE_z\"] = 0\n",
    "            analysis_data[model_name][f\"{n_pred}_pred\"][\"SSIM\"] = 0\n",
    "            analysis_data[model_name][f\"{n_pred}_pred\"][\"PSNR\"] = 0\n",
    "            for ii in range(T, total_len - n_pred):\n",
    "                x_img_i = imgs[:, (ii - 1):(ii + 1)] \n",
    "                x_img_i = frame_stack(x_img_i, frames=T)[:, 0]\n",
    "\n",
    "                x_img_gt = imgs[:, (ii + n_pred - 1):(ii + n_pred + 1)]\n",
    "                x_img_gt = frame_stack(x_img_gt, frames=T)[:, 0]\n",
    "                \n",
    "                if model_args.context==\"initial_latent_state\":\n",
    "                    ctx_img = x_img_i  \n",
    "                else:\n",
    "                    ctx_img = None\n",
    "                \n",
    "                x_ft_i = data['haptic'][:, ii:(ii + 1)]\n",
    "                x_ft_gt = data['haptic'][:, (ii + n_pred):(ii + n_pred + 1)]\n",
    "\n",
    "                x_arm_i = data['arm'][:, ii:(ii + 1)]                 \n",
    "                x_arm_gt = data['arm'][:, (ii + n_pred):(ii + n_pred + 1)]\n",
    " \n",
    "                z_gt, mu_z_gt, var_z_gt, _ = encode(nets, x_img_gt, x_ft_gt, x_arm_gt, ctx_img)\n",
    "#                 print(\"GT z\", z_gt[0])\n",
    "#                 print(\"GT mu\", mu_z_gt[0])\n",
    "#                 print(\"GT var\", torch.diagonal(var_z_gt[0]))\n",
    "\n",
    "                u = actions[:, (ii + 1):(ii + n_pred + 1)]\n",
    "                    \n",
    "                # Encode\n",
    "                z_i, mu_z_i, var_z_i, context = encode(nets, x_img_i, x_ft_i, x_arm_i, ctx_img)\n",
    "                h_i = None\n",
    "                \n",
    "                # Predict\n",
    "                for jj in range(n_pred):\n",
    "                    z_ip1, mu_z_ip1, var_z_ip1, h_ip1 = nets[\"dyn\"](\n",
    "                        z_t=z_i, \n",
    "                        mu_t=mu_z_i, \n",
    "                        var_t=var_z_i, \n",
    "                        u=u[:, jj], \n",
    "                        h_0=h_i, \n",
    "                        single=True\n",
    "                    )\n",
    "                    z_i, mu_z_i, var_z_i, h_i = z_ip1, mu_z_ip1, var_z_ip1, h_ip1    \n",
    "                \n",
    "#                 print(\"PRED z\", z_ip1[0])\n",
    "#                 print(\"PRED mu\", mu_z_ip1[0])\n",
    "#                 print(\"PRED var\", torch.diagonal(var_z_ip1[0]))\n",
    "\n",
    "                # Decode \n",
    "                z_all_dec = []\n",
    "                z_hat = z_ip1\n",
    "                z_all_dec.append(z_hat)\n",
    "\n",
    "                if model_args.context==\"initial_latent_state\":\n",
    "                    z_img_context = context[\"z_img_context\"]\n",
    "                    z_img_context_rep = z_img_context\n",
    "                    z_all_dec.append(z_img_context_rep)\n",
    "    \n",
    "                z_cat_dec = torch.cat(z_all_dec, dim=1)\n",
    "                x_hat = nets[\"img_dec\"](z_cat_dec)\n",
    "\n",
    "                x_hat = x_hat[:, 0:1].view(x_hat.shape[0], -1).cpu().numpy()\n",
    "                x_img_gt = x_img_gt[:, 0:1].view(x_img_gt.shape[0], -1).cpu().numpy()\n",
    "                z_hat = z_hat.cpu().numpy()\n",
    "                z_gt = z_gt.cpu().numpy()\n",
    "\n",
    "                n = x_img_gt.shape[0]\n",
    "\n",
    "                print(\"prediction length\", n_pred, \"starting position\", ii,\n",
    "                    \"mse img: \", (np.sum((x_img_gt - x_hat)**2) / n),\n",
    "                    \"mse z: \", (np.sum((z_gt - z_hat)**2) / n))\n",
    "                print()\n",
    "\n",
    "                analysis_data[model_name][f\"{n_pred}_pred\"][\"MSE_x\"] += (np.sum((x_img_gt - x_hat)**2) / n)\n",
    "                analysis_data[model_name][f\"{n_pred}_pred\"][\"MSE_z\"] += (np.sum((z_gt - z_hat)**2) / n)\n",
    "\n",
    "                batch_ssim = 0\n",
    "                batch_psnr = 0\n",
    "                for jj in range(n):\n",
    "                    batch_ssim += ssim(\n",
    "                        x_img_gt[jj].reshape(64,64), \n",
    "                        x_hat[jj].reshape(64,64), \n",
    "                        data_range=1.0\n",
    "                    )\n",
    "                    analysis_data[model_name][f\"{n_pred}_pred\"][\"PSNR\"] += psnr(\n",
    "                        x_img_gt[jj].reshape(64,64), \n",
    "                        x_hat[jj].reshape(64,64), \n",
    "                        data_range=1.0\n",
    "                    )\n",
    "                    \n",
    "                analysis_data[model_name][f\"{n_pred}_pred\"][\"SSIM\"] += (batch_ssim / n)\n",
    "                analysis_data[model_name][f\"{n_pred}_pred\"][\"PSNR\"] += (batch_psnr / n)\n",
    "    \n",
    "            # Average MSE/SSIM/PSNR per image \n",
    "            for k in analysis_data[model_name][f\"{n_pred}_pred\"]:\n",
    "                analysis_data[model_name][f\"{n_pred}_pred\"][k] /= (total_len - n_pred - 1)\n",
    "    print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(16,12))\n",
    "\n",
    "def add_data_to_plot(name, data):\n",
    "    properties = name.split(\"_\")\n",
    "    \n",
    "    # Map model properties to line properties here\n",
    "    if \"vha\" in properties:\n",
    "        color = \"r\"\n",
    "    else:\n",
    "        color = \"b\"\n",
    "        \n",
    "#     if \"4step\" in properties:\n",
    "#         linestyle = \"-.\"\n",
    "#     else:\n",
    "#         linestyle = \"-\"\n",
    "        \n",
    "    if \"nl\" in properties:\n",
    "        marker = \"o\"\n",
    "    else:\n",
    "        marker = \"v\"\n",
    "    \n",
    "    plot_data = {'MSE_x': [], 'SSIM': [], 'PSNR': [], \"MSE_z\": []}\n",
    "    for k, v in data.items():\n",
    "        plot_data['MSE_x'].append(v['MSE_x'])\n",
    "        plot_data['SSIM'].append(v['SSIM'])\n",
    "        plot_data['PSNR'].append(v['PSNR'])\n",
    "        plot_data['MSE_z'].append(v['MSE_z'])\n",
    "\n",
    "    axs[0,0].plot(\n",
    "        list(range(1, len(plot_data['MSE_x']) + 1)), \n",
    "        plot_data['MSE_x'],\n",
    "        color=color, \n",
    "        linestyle=linestyle,\n",
    "        marker=marker\n",
    "    )\n",
    "    axs[0,1].plot(\n",
    "        list(range(1, len(plot_data['SSIM']) + 1)), \n",
    "        plot_data['SSIM'],\n",
    "        color=color, \n",
    "        linestyle=linestyle,\n",
    "        marker=marker\n",
    "    )\n",
    "    axs[1,0].plot(\n",
    "        list(range(1, len(plot_data['PSNR']) + 1)), \n",
    "        plot_data['PSNR'],\n",
    "        color=color,\n",
    "        linestyle=linestyle,\n",
    "        marker=marker\n",
    "    )\n",
    "    axs[1,1].plot(\n",
    "        list(range(1, len(plot_data['MSE_z']) + 1)), \n",
    "        plot_data['MSE_z'],\n",
    "        color=color,\n",
    "        linestyle=linestyle,\n",
    "        marker=marker\n",
    "    )\n",
    "\n",
    "axs[0,0].set_title(\"MSE (Image)\")\n",
    "axs[0,1].set_title(\"SSIM\")\n",
    "axs[1,0].set_title(\"PSNR\")\n",
    "axs[1,1].set_title(\"MSE (Latent)\")\n",
    "\n",
    "for k, v in analysis_data.items():\n",
    "    add_data_to_plot(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
